#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
êµ­ê°€ë²•ë ¹ì •ë³´ OPEN API - ì „ì²´ ì—”ë“œí¬ì¸íŠ¸ ìƒ˜í”Œ í˜¸ì¶œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
Version 2.0.0 (2025-01-11)
- ëª©ë¡ í˜ì´ì§€ë¥¼ íŒŒì‹±í•´ ëª¨ë“  ê°€ì´ë“œ(guideResult) í˜ì´ì§€ë¥¼ ìˆ˜ì§‘
- ê° ê°€ì´ë“œì—ì„œ 'ìƒ˜í”Œ URL'ì„ ìë™ ì¶”ì¶œ í›„ í˜¸ì¶œ
- API_law.yaml ì„¤ì • íŒŒì¼ ì§€ì› (ì—†ìœ¼ë©´ OC=test ì‚¬ìš©)
- ë¯¼ê°ì •ë³´ ìë™ ì œê±° ê¸°ëŠ¥ ì¶”ê°€
- ì‚¬ìš© ì‹œ: python test_gpt.py
"""
import os
import re
import time
import json
import html
import yaml
import random
import pathlib
import urllib.parse as urlparse
from datetime import datetime
from typing import Dict, Any

import requests
from bs4 import BeautifulSoup

GUIDE_LIST_URL = "https://open.law.go.kr/LSO/openApi/guideList.do"
GUIDE_BASE     = "https://open.law.go.kr"
USER_AGENT     = "Mozilla/5.0 (compatible; OpenLawAPITester/2.0; +https://open.law.go.kr)"
TIMEOUT_SEC    = 15
SLEEP_SEC      = 0.5   # ì„œë²„ ë°°ë ¤ìš©(ë¬´ë¦¬í•œ ë™ì‹œí˜¸ì¶œ ì§€ì–‘)

session = requests.Session()
session.headers.update({"User-Agent": USER_AGENT})

def ensure_dir(p: str):
    pathlib.Path(p).mkdir(parents=True, exist_ok=True)

def load_config() -> Dict:
    """YAML ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    config_file = 'API_law.yaml'
    
    if not os.path.exists(config_file):
        print("â„¹ï¸ API_law.yaml íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. test í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return {'email_id': 'test'}
    
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            content = f.read()
            if content.startswith('---'):
                parts = content.split('---')
                if len(parts) >= 3:
                    yaml_content = parts[2].strip()
                else:
                    yaml_content = content
            else:
                yaml_content = content
                
            config = yaml.safe_load(yaml_content)
            return config if config else {'email_id': 'test'}
    except Exception as e:
        print(f"âš ï¸ YAML íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return {'email_id': 'test'}

def sanitize_data(data: Any) -> Any:
    """
    ë¯¼ê°ì •ë³´ ì œê±° (ì¬ê·€ì )
    - OC íŒŒë¼ë¯¸í„° ì œê±°
    - ì´ë©”ì¼ íŒ¨í„´ ë§ˆìŠ¤í‚¹
    """
    if isinstance(data, dict):
        cleaned = {}
        for key, value in data.items():
            # OC ê´€ë ¨ í•„ë“œ ì œê±°
            if key.upper() == 'OC' or key == 'email_id':
                continue
            # URLì—ì„œ OC íŒŒë¼ë¯¸í„° ì œê±°
            if key in ['url', 'guide_url', 'sample_url'] and isinstance(value, str):
                value = re.sub(r'[?&]OC=[^&]*', '', value)
            # ê°’ì—ì„œë„ ì¬ê·€ì ìœ¼ë¡œ ì œê±°
            cleaned[key] = sanitize_data(value)
        return cleaned
    elif isinstance(data, list):
        return [sanitize_data(item) for item in data]
    elif isinstance(data, str):
        # ì´ë©”ì¼ íŒ¨í„´ ë§ˆìŠ¤í‚¹
        return re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '***@***.***', data)
    else:
        return data

def slugify(text: str) -> str:
    text = re.sub(r"\s+", "_", text.strip())
    text = re.sub(r"[^\w\-\.]+", "_", text, flags=re.UNICODE)
    return text.strip("_")[:120]

def get(url: str) -> requests.Response:
    return session.get(url, timeout=TIMEOUT_SEC, allow_redirects=True)

def find_all_guide_pages(list_html: str):
    """
    guideList í˜ì´ì§€ì—ì„œ ëª¨ë“  guideResult.do?htmlName=... ë§í¬ë¥¼ ì¶”ì¶œ
    - onclick="javascript:openApiGuide('...')" í˜•ì‹ìœ¼ë¡œ ë˜ì–´ ìˆìŒ
    """
    # onclick íŒ¨í„´ì—ì„œ htmlName ì¶”ì¶œ
    html_names = set(re.findall(r"openApiGuide\(['\"]([A-Za-z0-9_]+)['\"]\)", list_html))
    
    # guideResult.do?htmlName= íŒ¨í„´ë„ ì¶”ê°€ë¡œ í™•ì¸
    html_names.update(re.findall(r"guideResult\.do\?htmlName=([A-Za-z0-9_]+)", list_html))
    
    guides = [f"{GUIDE_BASE}/LSO/openApi/guideResult.do?htmlName={name}" for name in sorted(html_names)]
    return guides

def parse_guide_page(url: str):
    """
    ê°€ì´ë“œ(detail) í˜ì´ì§€ì—ì„œ ì œëª©/ìš”ì²­URL/ìƒ˜í”ŒURLë“¤ ì¶”ì¶œ
    - í˜ì´ì§€ êµ¬ì¡°ê°€ ë‹¤ì–‘í•  ìˆ˜ ìˆì–´ ì™„í™”ëœ íŒŒì„œ ì‚¬ìš©
    """
    r = get(url)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")
    text = soup.get_text("\n", strip=True)

    # ëŒ€ëµì ì¸ ì œëª©
    title = None
    h = soup.find(["h2", "h3"])
    if h:
        title = h.get_text(strip=True)
    if not title:
        # ë°±ì—…: ì²« ì¤„ì—ì„œ 'API' ë‹¨ì–´ í¬í•¨ ë¼ì¸
        for line in text.splitlines():
            if "API" in line or "ê°€ì´ë“œ" in line:
                title = line.strip()
                break
    if not title:
        title = url

    # ìš”ì²­ URL (í˜ì´ì§€ì— "ìš”ì²­ URL" íŒ¨í„´ ì¡´ì¬)
    req_urls = re.findall(r"(https?://[^\s]+?lawService\.do[^\s]*)", text) \
             + re.findall(r"(https?://[^\s]+?lawSearch\.do[^\s]*)", text)
    req_urls = [u.rstrip(") ,") for u in req_urls]

    # ìƒ˜í”Œ URL ì„¹ì…˜ì—ì„œ ë§í¬ ì¶”ì¶œ (a[href] ìš°ì„ )
    sample_urls = []
    for a in soup.find_all("a", href=True):
        href = html.unescape(a["href"])
        if "law.go.kr/DRF/" in href or "/DRF/" in href or "lawService.do" in href or "lawSearch.do" in href:
            if href.startswith("/"):
                href = urlparse.urljoin("http://www.law.go.kr", href)
            sample_urls.append(href)

    # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì† URLë„ í›‘ê¸°
    text_urls = re.findall(r"(https?://[^\s\"'<>]+\blaw(?:Service|Search)\.do[^\s\"'<>]*)", text)
    sample_urls.extend(text_urls)

    # ì •ë¦¬
    def norm(u: str) -> str:
        u = u.replace("&amp;", "&")
        u = u.strip().rstrip(").,]")
        return u
    sample_urls = [norm(u) for u in sample_urls]
    # ìƒ˜í”Œ URLë§Œ ë‚¨ê¸°ê³  ì¤‘ë³µ ì œê±°
    sample_urls = list(dict.fromkeys([u for u in sample_urls if "law.go.kr/DRF" in u]))

    return {
        "title": title,
        "guide_url": url,
        "request_urls": list(dict.fromkeys(req_urls)),
        "sample_urls": sample_urls,
        "html": r.text,
    }

def ensure_oc_param(u: str, oc_value: str = None) -> str:
    """
    ìƒ˜í”Œ URLì— OC íŒŒë¼ë¯¸í„°ê°€ ë¹ ì ¸ ìˆìœ¼ë©´ ì¶”ê°€.
    ë˜ type íŒŒë¼ë¯¸í„°ê°€ ì „í˜€ ì—†ê³  lawService/lawSearchë©´ type=XMLì„ ë¶™ì—¬ì¤ë‹ˆë‹¤(ë³´ìˆ˜ì  ë³´ì •).
    """
    parsed = urlparse.urlparse(u)
    qs = urlparse.parse_qs(parsed.query, keep_blank_values=True)
    changed = False

    if "OC" not in qs:
        # ì„¤ì • íŒŒì¼ì´ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
        if oc_value:
            qs["OC"] = [oc_value]
        else:
            qs["OC"] = ["test"]  # ê¸°ë³¸ê°’
        changed = True

    # typeì´ ì „í˜€ ì—†ê³  DRF í˜¸ì¶œì´ë©´ XML ê¸°ë³¸
    if "type" not in qs and ("lawService.do" in parsed.path or "lawSearch.do" in parsed.path):
        qs["type"] = ["XML"]
        changed = True

    if changed:
        new_q = urlparse.urlencode({k: v[0] for k, v in qs.items()}, doseq=False)
        u = urlparse.urlunparse(parsed._replace(query=new_q))
    return u

def save_response(name_slug: str, resp: requests.Response, url: str, out_dir: str):
    ct = (resp.headers.get("Content-Type") or "").lower()
    # í™•ì¥ì ì¶”ì •
    if "json" in ct:
        ext = "json"
    elif "html" in ct:
        ext = "html"
    else:
        # ëŒ€ë¶€ë¶„ XML
        ext = "xml"
    safe_name = name_slug[:80]
    path = os.path.join(out_dir, f"{safe_name}.{ext}")
    
    try:
        # í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”© ì‹œë„
        content = resp.text
        # OC íŒŒë¼ë¯¸í„° ì œê±°
        content = re.sub(r'[?&]OC=[^&"\s]*', '', content)
        content = re.sub(r'<OC>[^<]*</OC>', '', content)  # XML íƒœê·¸ ë‚´ OC ì œê±°
        
        with open(path, "w", encoding="utf-8") as f:
            f.write(content)
    except:
        # ë°”ì´ë„ˆë¦¬ ë°ì´í„°ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì €ì¥
        with open(path, "wb") as f:
            f.write(resp.content)
    
    return path

def main():
    print("="*60)
    print("ğŸš€ ë²•ì œì²˜ Open API ì „ì²´ ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # ì„¤ì • ë¡œë“œ
    config = load_config()
    email_id = config.get('email_id', 'test')
    
    # ì´ë©”ì¼ì—ì„œ @ ì•ë¶€ë¶„ë§Œ ì¶”ì¶œ
    if '@' in email_id:
        email_id = email_id.split('@')[0]
    
    # ì‹¤í–‰ ì‹œê°„ ê¸°ì¤€ í´ë”ëª… ìƒì„±
    session_folder = datetime.now().strftime('%Y%m%d_%H%M%S')
    out_dir = f'_cache/{session_folder}'
    
    print(f"â„¹ï¸ ì‚¬ìš©í•  OC ê°’: {'***' if email_id != 'test' else 'test'}")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ í´ë”: {out_dir}")
    print()
    
    print(f"[1/4] ëª©ë¡ í˜ì´ì§€ ìš”ì²­ â†’ {GUIDE_LIST_URL}")
    r = get(GUIDE_LIST_URL)
    r.raise_for_status()
    guides = find_all_guide_pages(r.text)
    print(f"  - guideResult í˜ì´ì§€ ìˆ˜ì§‘: {len(guides)}ê°œ ë°œê²¬")

    ensure_dir(out_dir)
    meta = []
    fail_records = []

    for idx, g in enumerate(guides, 1):
        print(f"\n[2/4] ({idx}/{len(guides)}) ê°€ì´ë“œ ë¶„ì„ â†’ {g}")
        try:
            info = parse_guide_page(g)
        except Exception as e:
            print(f"  ! ê°€ì´ë“œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            fail_records.append({"stage": "parse", "guide": g, "error": str(e)})
            continue

        title = info["title"]
        samples = info["sample_urls"]
        print(f"  - ì œëª©: {title}")
        print(f"  - ìƒ˜í”Œ URL: {len(samples)}ê°œ ë°œê²¬")

        # ìƒ˜í”Œì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ìš”ì²­URLì„ ì°¸ê³ í•´ì„œ ë³´ìˆ˜ì ìœ¼ë¡œ 1ê±´ ë§Œë“¤ì–´ ë³¸ë‹¤(ê°€ëŠ¥í•  ë•Œë§Œ)
        if not samples and info["request_urls"]:
            # ìš”ì²­URLì´ search/service ì¸ì§€ ë³´ê³  ìµœì†Œ íŒŒë¼ë¯¸í„°ë¥¼ ë¶™ì¸ë‹¤.
            for req_url in info["request_urls"]:
                if "lawService.do" in req_url or "lawSearch.do" in req_url:
                    # ê·¸ëƒ¥ OC=test&type=XML ì¶”ê°€ (ì¼ë¶€ëŠ” ID ë“± ì¶”ê°€í•„ìˆ˜ â†’ 400/ì—ëŸ¬ ê°€ëŠ¥)
                    u = req_url
                    joiner = "&" if ("?" in u) else "?"
                    samples.append(f"{u}{joiner}OC=test&type=XML")
                    break

        # ìƒ˜í”Œ í˜¸ì¶œ
        if not samples:
            print("  - í˜¸ì¶œ ìƒ˜í”Œì´ ì—†ì–´ SKIP")
            meta.append({"guide": g, "title": title, "called": 0, "ok": 0})
            continue

        ok = 0
        for sidx, su in enumerate(samples, 1):
            url_fixed = ensure_oc_param(su, email_id)
            # ì¶œë ¥ì‹œ OC ê°’ ë§ˆìŠ¤í‚¹
            display_url = re.sub(r'OC=[^&]*', 'OC=***', url_fixed)
            print(f"    [{sidx}/{len(samples)}] CALL â†’ {display_url}")
            try:
                time.sleep(SLEEP_SEC + random.uniform(0, 0.3))
                resp = get(url_fixed)
                status = resp.status_code
                ctype  = resp.headers.get("Content-Type", "")
                size   = len(resp.content)
                if status == 200 and size > 0:
                    # ê°€ì´ë“œ ë²ˆí˜¸ë¥¼ í¬í•¨í•´ì„œ íŒŒì¼ëª… ì¤‘ë³µ ë°©ì§€
                    fname = save_response(slugify(f"{title}_{idx}_{sidx}"), resp, url_fixed, out_dir)
                    print(f"      âœ… {status} | {ctype} | {size} bytes â†’ {fname}")
                    ok += 1
                else:
                    print(f"      âŒ {status} | {ctype} | {size} bytes")
                    # URLì—ì„œ OC ì œê±° í›„ ì €ì¥
                    clean_url = re.sub(r'[?&]OC=[^&]*', '', url_fixed)
                    fail_records.append({"stage":"call","guide":g,"title":title,"url":clean_url,"status":status,"ctype":ctype,"size":size})
            except Exception as e:
                print(f"      âš ï¸ {type(e).__name__}: {e}")
                # URLì—ì„œ OC ì œê±° í›„ ì €ì¥
                clean_url = re.sub(r'[?&]OC=[^&]*', '', url_fixed)
                fail_records.append({"stage":"call","guide":g,"title":title,"url":clean_url,"error":str(e)})

        meta.append({"guide": g, "title": title, "called": len(samples), "ok": ok})

    # ê²°ê³¼ ìš”ì•½ ì €ì¥ (ë¯¼ê°ì •ë³´ ì œê±°)
    clean_meta = sanitize_data(meta)
    clean_fails = sanitize_data(fail_records)
    
    summary_path = os.path.join(out_dir, "í…ŒìŠ¤íŠ¸ìš”ì•½.json")
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump({
            "test_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "total_guides": len(meta),
            "meta": clean_meta, 
            "fail": clean_fails
        }, f, ensure_ascii=False, indent=2)
    print("\n[3/4] ìš”ì•½ ì €ì¥ â†’", summary_path)

    # ê°„ë‹¨ ìš”ì•½ í‘œ
    total_called = sum(m["called"] for m in meta)
    total_ok     = sum(m["ok"]     for m in meta)
    print(f"\n[4/4] ğŸ“Š ì „ì²´ ìš”ì•½")
    print("="*60)
    print(f"âœ… ì„±ê³µ: {total_ok}ê±´")
    print(f"âŒ ì‹¤íŒ¨: {total_called - total_ok}ê±´")
    print(f"ğŸ“š ê°€ì´ë“œ: {len(meta)}ê±´")
    print(f"ğŸ” ì „ì²´ í˜¸ì¶œ: {total_called}ê±´")
    
    if fail_records:
        print(f"\nâš ï¸ ì‹¤íŒ¨ ì˜ˆì‹œ (ìµœëŒ€ 3ê±´):")
        for rec in fail_records[:3]:
            print(f"  - {rec.get('title', 'Unknown')}: {rec.get('error', rec.get('status', 'Unknown error'))}")
    
    print(f"\nğŸ’¾ ëª¨ë“  ê²°ê³¼ê°€ {out_dir}/ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()

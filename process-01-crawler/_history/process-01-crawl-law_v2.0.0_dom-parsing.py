#!/usr/bin/env python3
"""
ë²•ì œì²˜ ë²•ì¸ì„¸ë²• í¬ë¡¤ë§ - Selenium 4 ìë™ ë“œë¼ì´ë²„
Version 2.0.0 (2025-01-11)
- Selenium 4.6+ ë‚´ì¥ Selenium Manager ì‚¬ìš©
- ChromeDriver ë³„ë„ ì„¤ì¹˜ ë¶ˆí•„ìš”
- iframe ì²˜ë¦¬ ì¶”ê°€

Version 1.0.0 (2025-01-11)  
- ì´ˆê¸° ë²„ì „ ì‘ì„±
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import json
import time
import re
import os

class KoreanTaxLawCrawler:
    def __init__(self, headless=False):
        self.setup_driver(headless)
    
    def setup_driver(self, headless):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì • - Selenium 4 ìë™ ê´€ë¦¬"""
        chrome_options = Options()
        if headless:
            chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        
        # Selenium 4ëŠ” ìë™ìœ¼ë¡œ ChromeDriverë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ê´€ë¦¬
        print("ğŸ“¥ ChromeDriver ìë™ ì„¤ì • ì¤‘...")
        self.driver = webdriver.Chrome(options=chrome_options)
        self.wait = WebDriverWait(self.driver, 10)
        print("âœ… Chrome ë¸Œë¼ìš°ì € ì‹œì‘\n")
    
    def crawl_law(self, url):
        """ë²•ë ¹ í˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            print(f"ğŸ“– í˜ì´ì§€ ì ‘ì†: {url}")
            self.driver.get(url)
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            print("â³ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (10ì´ˆ)...")
            time.sleep(10)
            
            # iframe ì²˜ë¦¬
            try:
                print("ğŸ”„ iframe ê°ì§€ ì¤‘...")
                iframe = self.driver.find_element(By.ID, 'lawService')
                self.driver.switch_to.frame(iframe)
                print("âœ… iframe ë‚´ë¶€ë¡œ ì „í™˜ ì™„ë£Œ")
                time.sleep(3)  # iframe ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
            except:
                print("âš ï¸ iframe ì—†ìŒ, ë©”ì¸ í˜ì´ì§€ì—ì„œ ê³„ì†")
            
            # ì½˜í…ì¸  ì¶”ì¶œ
            return self.extract_law_data()
                
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
        finally:
            self.driver.quit()
            print("\në¸Œë¼ìš°ì €ë¥¼ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")
    
    def extract_law_data(self):
        """ë²•ë ¹ ë°ì´í„° ì¶”ì¶œ - ì¢Œì¸¡ ì¡°ë¬¸ ë¦¬ìŠ¤íŠ¸ DOM í™œìš©"""
        print("\nğŸ” ë²•ë ¹ ë‚´ìš© ì¶”ì¶œ ì¤‘...")
        
        law_data = {
            'title': '',
            'articles': [],
            'raw_text': '',
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        try:
            # ì „ì²´ í…ìŠ¤íŠ¸ ë°±ì—…ìš©ìœ¼ë¡œ ì €ì¥
            body = self.driver.find_element(By.TAG_NAME, 'body')
            law_data['raw_text'] = body.text
            
            # ì œëª© ì¶”ì¶œ
            title_pattern = r'ë²•ì¸ì„¸ë²•\s*\[ì‹œí–‰[^\]]+\]'
            title_match = re.search(title_pattern, law_data['raw_text'])
            if title_match:
                law_data['title'] = title_match.group().strip()
                print(f"ğŸ“Œ ì œëª©: {law_data['title']}")
            
            # ì¢Œì¸¡ ì¡°ë¬¸ ë¦¬ìŠ¤íŠ¸ ìŠ¤ìº”
            print("  ì¢Œì¸¡ ì¡°ë¬¸ ëª©ë¡ ìŠ¤ìº” ì¤‘...")
            try:
                # XPathë¡œ ì¢Œì¸¡ ë¶„í• ì˜ì—­ì˜ ì¡°ë¬¸ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
                article_list = self.driver.find_element(By.XPATH, '/html/body/form[2]/div[2]/div[1]/div[2]/ul/li/div')
                
                # í•˜ìœ„ ëª¨ë“  ì¡°ë¬¸ ë§í¬ ìš”ì†Œ ì°¾ê¸°
                article_links = article_list.find_elements(By.TAG_NAME, 'a')
                print(f"  {len(article_links)}ê°œ ì¡°ë¬¸ ë§í¬ ë°œê²¬")
                
                for link in article_links:
                    text = link.text.strip()
                    if text and 'ì œ' in text and 'ì¡°' in text:
                        # ì¡°ë¬¸ í…ìŠ¤íŠ¸ íŒŒì‹±
                        # ì œXì¡°(ì œëª©) ë˜ëŠ” ì œXì¡° ì œëª© í˜•ì‹ ì²˜ë¦¬
                        match = re.match(r'ì œ(\d+(?:ì˜\d+)?)ì¡°[\s\(ï¼ˆ]?([^)ï¼‰\n]*)', text)
                        if match:
                            article_num = match.group(1)
                            article_title = match.group(2).strip()
                            
                            # ë§ì¤„ì„í‘œê°€ ìˆìœ¼ë©´ ì œê±°í•˜ê³  í‘œì‹œ
                            if '...' in article_title:
                                article_title = article_title.replace('...', '').strip()
                                # ì‹¤ì œ ì „ì²´ ì œëª©ì€ ë³¸ë¬¸ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨ì„ í‘œì‹œ
                                article_title += ' [ì¶•ì•½ë¨]'
                            
                            law_data['articles'].append({
                                'number': article_num,
                                'title': article_title,
                                'href': link.get_attribute('href')  # ë§í¬ ì €ì¥
                            })
                
            except Exception as e:
                print(f"  XPath ì¡°ë¬¸ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                
                # ëŒ€ì²´ ë°©ë²•: CSS ì„ íƒìë¡œ ì‹œë„
                try:
                    article_links = self.driver.find_elements(By.CSS_SELECTOR, 'div.left_menu a, ul.tree_menu a, a[onclick*="goJoView"]')
                    print(f"  CSS ì„ íƒìë¡œ {len(article_links)}ê°œ ë§í¬ ë°œê²¬")
                    
                    for link in article_links:
                        text = link.text.strip()
                        if text and 'ì œ' in text and 'ì¡°' in text:
                            match = re.match(r'ì œ(\d+(?:ì˜\d+)?)ì¡°[\s\(ï¼ˆ]?([^)ï¼‰\n]*)', text)
                            if match:
                                law_data['articles'].append({
                                    'number': match.group(1),
                                    'title': match.group(2).strip()
                                })
                except:
                    pass
            
            # ì¡°ë¬¸ì´ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
            if not law_data['articles']:
                print("  DOM ì¶”ì¶œ ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì¶œ")
                main_start = law_data['raw_text'].find('ì œ1ì¡°(ëª©ì )')
                if main_start >= 0:
                    main_text = law_data['raw_text'][main_start:]
                    article_pattern = r'ì œ(\d+(?:ì˜\d+)?)ì¡°\(([^)]+)\)'
                    articles = re.findall(article_pattern, main_text)
                    
                    seen = set()
                    for article_num, article_title in articles:
                        if article_num not in seen:
                            seen.add(article_num)
                            law_data['articles'].append({
                                'number': article_num,
                                'title': article_title.strip()
                            })
            
            # ì¡°ë¬¸ ë²ˆí˜¸ìˆœ ì •ë ¬
            law_data['articles'].sort(key=lambda x: (
                int(x['number'].split('ì˜')[0]),
                int(x['number'].split('ì˜')[1]) if 'ì˜' in x['number'] else 0
            ))
            
            print(f"âœ… {len(law_data['articles'])}ê°œ ì¡°ë¬¸ ë°œê²¬")
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return law_data
    
    def save_results(self, data):
        """ê²°ê³¼ ì €ì¥ - ìºì‹œ íŒŒì¼ í˜•ì‹"""
        if not data:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('cache', exist_ok=True)
        
        # JSON ìºì‹œ íŒŒì¼
        json_file = 'cache/01_law_data_cache.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… {json_file} ì €ì¥")
        
        # í…ìŠ¤íŠ¸ ìºì‹œ íŒŒì¼
        txt_file = 'cache/01_law_text_cache.txt'
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write(data.get('raw_text', ''))
        print(f"âœ… {txt_file} ì €ì¥")
        
        # ìš”ì•½ ë¦¬í¬íŠ¸
        self.print_summary(data)
    
    def print_summary(self, data):
        """í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½"""
        print("\n" + "="*60)
        print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        print(f"ì œëª©: {data.get('title', 'ë¯¸í™•ì¸')}")
        print(f"ì¡°ë¬¸ ìˆ˜: {len(data.get('articles', []))}ê°œ")
        print(f"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(data.get('raw_text', '')):,} ì")
        print(f"í¬ë¡¤ë§ ì‹œê°„: {data.get('timestamp', '')}")
        
        if data.get('articles'):
            print(f"\nì²˜ìŒ 5ê°œ ì¡°ë¬¸:")
            for article in data['articles'][:5]:
                print(f"  - ì œ{article['number']}ì¡° ({article['title']})")

def main():
    print("ğŸš€ ë²•ì¸ì„¸ë²• í¬ë¡¤ë§ ì‹œì‘")
    print("="*60)
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” (headless=Falseë¡œ ë¸Œë¼ìš°ì € í‘œì‹œ)
    crawler = KoreanTaxLawCrawler(headless=False)
    
    # ë²•ì¸ì„¸ë²• URL
    url = "https://www.law.go.kr/ë²•ë ¹/ë²•ì¸ì„¸ë²•"
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    result = crawler.crawl_law(url)
    
    if result:
        crawler.save_results(result)
        print("\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!")
    else:
        print("\nâŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
        print("ëŒ€ì•ˆ: ë²•ì œì²˜ ì‚¬ì´íŠ¸ì—ì„œ HWP/PDF ì§ì ‘ ë‹¤ìš´ë¡œë“œ")

if __name__ == "__main__":
    main()
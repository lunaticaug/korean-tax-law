#!/usr/bin/env python3
"""
ë²•ì œì²˜ ë²•ì¸ì„¸ë²• ì¸í„°ë™í‹°ë¸Œ ë¸Œë¼ìš°ì € í¬ë¡¤ë§
Version 1.0.0 (2025-01-11)
- ë¸Œë¼ìš°ì €ë¥¼ ì—´ì–´ë‘ê³  ì‚¬ìš©ìê°€ ì§ì ‘ í˜ì´ì§€ í™•ì¸ í›„ í¬ë¡¤ë§
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
# from webdriver_manager.chrome import ChromeDriverManager
import json
import time
import re

class InteractiveBrowserScraper:
    def __init__(self):
        self.setup_driver()
        self.data = None
    
    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì • (í™”ë©´ í‘œì‹œ)"""
        chrome_options = Options()
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_experimental_option('detach', True)  # ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ í›„ì—ë„ ë¸Œë¼ìš°ì € ìœ ì§€
        
        # service = Service(ChromeDriverManager().install())
        # self.driver = webdriver.Chrome(service=service, options=chrome_options)
        print("âœ… Chrome ë¸Œë¼ìš°ì €ê°€ ì—´ë ¸ìŠµë‹ˆë‹¤.")
    
    def open_law_page(self, url):
        """ë²•ë ¹ í˜ì´ì§€ ì—´ê¸°"""
        print(f"\nğŸ“– í˜ì´ì§€ ë¡œë”© ì¤‘: {url}")
        self.driver.get(url)
        print("â³ í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
        
    def wait_for_user_confirmation(self):
        """ì‚¬ìš©ì í™•ì¸ ëŒ€ê¸°"""
        print("\n" + "="*60)
        print("ğŸ‘€ ë¸Œë¼ìš°ì €ì—ì„œ í˜ì´ì§€ë¥¼ í™•ì¸í•˜ì„¸ìš”:")
        print("1. ë²•ë ¹ ë‚´ìš©ì´ ëª¨ë‘ í‘œì‹œë˜ì—ˆëŠ”ì§€ í™•ì¸")
        print("2. í•„ìš”ì‹œ ìŠ¤í¬ë¡¤í•˜ì—¬ ì „ì²´ ë‚´ìš© ë¡œë“œ")
        print("3. íŒì—…ì´ ìˆë‹¤ë©´ ë‹«ê¸°")
        print("="*60)
        
        while True:
            user_input = input("\nì¤€ë¹„ë˜ì…¨ë‚˜ìš”? (y: í¬ë¡¤ë§ ì‹œì‘, n: ì·¨ì†Œ, r: í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨): ").lower()
            if user_input == 'y':
                return True
            elif user_input == 'n':
                return False
            elif user_input == 'r':
                print("í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•©ë‹ˆë‹¤...")
                self.driver.refresh()
                time.sleep(3)
            else:
                print("y, n, ë˜ëŠ” rì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    def extract_content(self):
        """í˜„ì¬ í˜ì´ì§€ì—ì„œ ë²•ë ¹ ë‚´ìš© ì¶”ì¶œ"""
        print("\nğŸ” ë²•ë ¹ ë‚´ìš©ì„ ì¶”ì¶œí•˜ëŠ” ì¤‘...")
        
        # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì½˜í…ì¸  ì¶”ì¶œ ì‹œë„
        extraction_methods = [
            self.extract_by_id,
            self.extract_by_class,
            self.extract_by_xpath,
            self.extract_full_page
        ]
        
        for method in extraction_methods:
            result = method()
            if result and (result.get('articles') or result.get('raw_text')):
                print(f"âœ… {method.__name__} ë°©ë²•ìœ¼ë¡œ ì¶”ì¶œ ì„±ê³µ!")
                self.data = result
                return True
        
        print("âŒ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨")
        return False
    
    def extract_by_id(self):
        """IDë¡œ ì¶”ì¶œ"""
        try:
            for elem_id in ['conScroll', 'lawmunView', 'content', 'lawContent']:
                try:
                    element = self.driver.find_element(By.ID, elem_id)
                    if element:
                        return self.parse_element(element)
                except:
                    continue
        except:
            pass
        return None
    
    def extract_by_class(self):
        """Classë¡œ ì¶”ì¶œ"""
        try:
            for class_name in ['lawcon', 'law_contents', 'content_body', 'law-content']:
                try:
                    element = self.driver.find_element(By.CLASS_NAME, class_name)
                    if element:
                        return self.parse_element(element)
                except:
                    continue
        except:
            pass
        return None
    
    def extract_by_xpath(self):
        """XPathë¡œ ì¶”ì¶œ"""
        try:
            xpaths = [
                '//div[contains(@class, "law")]',
                '//div[contains(@id, "law")]',
                '//article',
                '//main'
            ]
            for xpath in xpaths:
                try:
                    element = self.driver.find_element(By.XPATH, xpath)
                    if element:
                        return self.parse_element(element)
                except:
                    continue
        except:
            pass
        return None
    
    def extract_full_page(self):
        """ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            body = self.driver.find_element(By.TAG_NAME, 'body')
            return self.parse_element(body)
        except:
            return None
    
    def parse_element(self, element):
        """ìš”ì†Œì—ì„œ ë²•ë ¹ ë°ì´í„° íŒŒì‹±"""
        law_data = {
            'title': '',
            'articles': [],
            'raw_text': '',
            'html': ''
        }
        
        # HTMLê³¼ í…ìŠ¤íŠ¸ ì €ì¥
        law_data['html'] = element.get_attribute('innerHTML')
        law_data['raw_text'] = element.text
        
        # ì œëª© ì¶”ì¶œ
        title_pattern = r'ë²•ì¸ì„¸ë²•\s*(?:\[.*?\])?'
        title_match = re.search(title_pattern, law_data['raw_text'])
        if title_match:
            law_data['title'] = title_match.group()
        
        # ì¡°ë¬¸ ì¶”ì¶œ
        article_pattern = r'ì œ(\d+(?:ì˜\d+)?)ì¡°[\s\(ï¼ˆ](.*?)[\)ï¼‰]?\s*(.*?)(?=ì œ\d+(?:ì˜\d+)?ì¡°|$)'
        articles = re.findall(article_pattern, law_data['raw_text'], re.DOTALL)
        
        for article_num, article_title, article_content in articles:
            law_data['articles'].append({
                'number': article_num,
                'title': article_title.strip(),
                'content': article_content.strip()[:500]  # ì²˜ìŒ 500ìë§Œ ì €ì¥
            })
        
        return law_data
    
    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        if not self.data:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # JSON ì €ì¥
        with open('tax_law_interactive.json', 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
        print("âœ… tax_law_interactive.json ì €ì¥ ì™„ë£Œ")
        
        # ì›ë³¸ í…ìŠ¤íŠ¸ ì €ì¥
        with open('tax_law_raw.txt', 'w', encoding='utf-8') as f:
            f.write(self.data.get('raw_text', ''))
        print("âœ… tax_law_raw.txt ì €ì¥ ì™„ë£Œ")
        
        # HTML ì €ì¥
        with open('tax_law.html', 'w', encoding='utf-8') as f:
            f.write(self.data.get('html', ''))
        print("âœ… tax_law.html ì €ì¥ ì™„ë£Œ")
        
        # ìš”ì•½ ì¶œë ¥
        print(f"\nğŸ“Š ì¶”ì¶œ ê²°ê³¼:")
        print(f"- ì œëª©: {self.data.get('title', 'ë¯¸í™•ì¸')}")
        print(f"- ì¶”ì¶œëœ ì¡°ë¬¸ ìˆ˜: {len(self.data.get('articles', []))}")
        print(f"- ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(self.data.get('raw_text', ''))} ì")
    
    def keep_browser_open(self):
        """ë¸Œë¼ìš°ì € ìœ ì§€ ì˜µì…˜"""
        print("\n" + "="*60)
        print("ë¸Œë¼ìš°ì € ì˜µì…˜:")
        print("1. ë¸Œë¼ìš°ì € ìœ ì§€ (ìˆ˜ë™ìœ¼ë¡œ ë‹«ê¸°)")
        print("2. ë¸Œë¼ìš°ì € ë‹«ê¸°")
        print("="*60)
        
        choice = input("ì„ íƒ (1 ë˜ëŠ” 2): ")
        if choice == '2':
            self.driver.quit()
            print("ë¸Œë¼ìš°ì €ë¥¼ ë‹«ì•˜ìŠµë‹ˆë‹¤.")
        else:
            print("ë¸Œë¼ìš°ì €ê°€ ì—´ë ¤ìˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ë‹«ì•„ì£¼ì„¸ìš”.")

def main():
    print("ğŸš€ ì¸í„°ë™í‹°ë¸Œ ë¸Œë¼ìš°ì € í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("="*60)
    
    scraper = InteractiveBrowserScraper()
    
    # URL ì…ë ¥ ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
    url = input("\në²•ë ¹ URL ì…ë ¥ (Enter: ë²•ì¸ì„¸ë²•): ").strip()
    if not url:
        url = "https://www.law.go.kr/ë²•ë ¹/ë²•ì¸ì„¸ë²•"
    
    # í˜ì´ì§€ ì—´ê¸°
    scraper.open_law_page(url)
    
    # ì‚¬ìš©ì í™•ì¸ ëŒ€ê¸°
    if scraper.wait_for_user_confirmation():
        # ì½˜í…ì¸  ì¶”ì¶œ
        if scraper.extract_content():
            scraper.save_results()
        else:
            print("\nì¶”ì¶œ ì‹¤íŒ¨. ë‹¤ìŒì„ ì‹œë„í•´ë³´ì„¸ìš”:")
            print("1. í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸")
            print("2. ë‹¤ë¥¸ ë²•ë ¹ ì‚¬ì´íŠ¸ ì‚¬ìš©")
            print("3. PDF/HWP ë‹¤ìš´ë¡œë“œ ì˜µì…˜ í™•ì¸")
    else:
        print("í¬ë¡¤ë§ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ë¸Œë¼ìš°ì € ìœ ì§€ ì—¬ë¶€ ì„ íƒ
    scraper.keep_browser_open()

if __name__ == "__main__":
    main()